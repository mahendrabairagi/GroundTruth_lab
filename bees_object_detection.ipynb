{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Ground Truth and Object Detection\n",
    "## Workshop Guide\n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction to dataset](#introduction)\n",
    "2. [Labeling with SageMaker Ground Truth](#groundtruth)\n",
    "3. [Reviewing labeling results](#review)\n",
    "4. [Training an Object Detection model](#training)\n",
    "5. [Review of Training Results](#review_training)\n",
    "6. [Model Tuning](#model_tuning)\n",
    "7. [Cleanup](#cleanup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap embedded images\n",
    "\n",
    "Download images used in the rest of this guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-remars/guide.zip .\n",
    "!unzip -qo guide.zip guide/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Introduction to dataset\n",
    "In this workshop we will use a dataset from the [inaturalist.org] This dataset contains 500 images of bees that have been uploaded by inaturalist users for the purposes of recording the observation and identification. We only used images that their users have licensed under [CC0](https://creativecommons.org/share-your-work/public-domain/cc0/) license. For your convenience, we have placed the dataset in S3 in a single zip archive here: http://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/DIG-TF-200-MLBEES-10-EN/dataset.zip \n",
    "\n",
    "First, download and unzip the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://aws-tc-largeobjects.s3-us-west-2.amazonaws.com/DIG-TF-200-MLBEES-10-EN/dataset.zip \n",
    "!unzip -qo dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive contains the following structure: 500 `.jpg` image files, a manifest file (to be explained later) and 10 test images in the `test` subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l dataset.zip | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's upload this dataset to your own S3 bucket in preparation for labeling and training using Amazon SageMaker. For this workshop we will be using `us-west-2` region, so your bucket needs to be in this region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket must be created in us-west-2 (Oregon) region\n",
    "BUCKET = '<your bucket name>'\n",
    "PREFIX = 'input' # this is the root path to your working space, feel to use a different path\n",
    "# For example, in screenshots below we assumed the following configuration, but you need to supply your own bucket\n",
    "#BUCKET = 'sagemaker-remars'\n",
    "#PREFIX = 'testing/upload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync --exclude=\"*\" --include=\"[0-9]*.jpg\" . s3://$BUCKET/$PREFIX/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling with SageMaker Ground Truth <a name=\"groundtruth\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to run your first labeling with Amazon SageMaker Ground Truth. We have already labeled the larger dataset, and you will be using this labeled dataset to train your model. This exercise is meant to only demonstrate how you would use SageMaker Ground Truth to label your own datasets, so you will only get label  small sample labeled. Belwo we walk through the four steps for creating and kicking off a labeling job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Navigate to the SageMaker Ground Truth console.\n",
    "\n",
    "In the AWS Management Console, navigate to Amazon SageMaker and then select 'Labeling jobs' option under the 'Ground Truth' menu. Remember to make sure you are in the Oregon region.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step1a.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "To kick off a Labeling, click the 'Create labeling job' button.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step1b.png\" width=\"600\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up input and output dataset locations\n",
    "\n",
    "Let's begin by giving a name to the labeling job. Since this labeling job is only meant to be a sample, let's name it 'Sample-labeling-job'. \n",
    "\n",
    "Then, let's give the path to the input dataset location. A labeling job takes an input manifest as the input dataset. An input manifest effectively points to the dataset objects in the dataset.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step2a.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Realize that we have not yet created the input manifest. Instead above, we only uploaded 500 images. Fortunately, SageMaker Ground Truth provides a cool feature that creates the input manifest for you. \n",
    "\n",
    "To use this feature, click on the 'Create manifest file' link and provide the path to the uploaded images: s3://sagemaker-remars/testing/upload/. Select 'Images' as the 'Data type' and then select 'Create' to kick off the process. Once the manifest creation process has completed, select 'Use this manifest'. \n",
    "\n",
    "<img src=\"guide/Tutorial-Step2b.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Next, let's fill out the output dataset location. This is the S3 location where SageMaker Ground Truth will output all of the  labels generated as well as any intermediate data. We will opt to simply point to the 'sagemaker-remars' S3 bucket as the output location. \n",
    "\n",
    "Then, we will provide the IAM Role that allows SageMaker Ground Truth to read and write to your S3 locations. The easiest option is 'Create a new role' and specifically provide access to the 'sagemaker-remars' S3 bucket.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step2c.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "There are currently 500 images in our input dataset. We do not want to send all 500 images to the labeling job (for time and cost reasons). Instead, we want to randomly sample 2% of the images (or 10 images). SageMaker Ground Truth makes it easy to perform this sampling.\n",
    "\n",
    "Click on 'Additional configuration', and click on the 'Random sample' option. Next, indicate you want to sample 2% of the dataset. Then to complete the process, click on 'Create subset'. This will create a new input manifest, and once it is created, click 'Use this subset'. \n",
    "\n",
    "<img src=\"guide/Tutorial-Step2d.png\" width=\"500\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select task type and worker type\n",
    "\n",
    "Under 'Task type', select the 'Bounding box' option. To complete this step, click 'Next' at the bottom of the page.\n",
    "\n",
    "<img src=\"guide/Tutorial-Step3a.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "For this labeling job, we want to use the 'Public' worker option. This option will direct the labeling job to the Amazon Mechanical Turk public workforce, which is globally distributed, 24/7 available.\n",
    "\n",
    "We will use the default 'Price per task', acknowledge that the dataset does not contain adult content, and acknowledge that we understand the data will be publically viewable. \n",
    "\n",
    "<img src=\"guide/Tutorial-Step3b.png\" width=\"600\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create the task instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most important step as it will directly impact the accuracy of the labels generated. We have discussed this topic in-depth in this blog post: https://aws.amazon.com/blogs/machine-learning/create-high-quality-instructions-for-amazon-sagemaker-ground-truth-labeling-jobs/\n",
    "\n",
    "Start with your primary call-to-action in the header; here we use: \"Draw a bounding box around the bee in this image.\" Then, let's define the label category on the right-hand side. This is \"bee\" in our case. \n",
    "\n",
    "Next, we want to provide visual examples of both good and bad labels. We have pre-created these examples, and you can directly insert these examples by clicking on the image icon and then pointing to the below public URLs:\n",
    "\n",
    "Good Example - https://s3.us-west-2.amazonaws.com/sagemaker-remars/bee-good-5535715.jpg \n",
    "\n",
    "Bad Example - https://s3.us-west-2.amazonaws.com/sagemaker-remars/bee-bad-5535715.jpg\n",
    "\n",
    "Then, we suggest providing some helper text to guide the workers for both good and bad examples. \n",
    "\n",
    "<img src=\"guide/Tutorial-Step4a.png\" width=\"600\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing labeling results\n",
    "<a name=\"reviewing\"></a>\n",
    "\n",
    "If your labeling job has not finished, we will review results of the previously completed labeling job using the provided augmented manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_job_name = 'bees-500'\n",
    "augmented_manifest_file = 'output.manifest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this cell ONLY if your labeling job successfully completed, otherwise skip to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the name of your job here\n",
    "labeling_job_name = 'Sample-labeling-job'\n",
    "\n",
    "import boto3\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "s3_output_path = client.describe_labeling_job(LabelingJobName=labeling_job_name)['OutputConfig']['S3OutputPath'].rstrip('/')\n",
    "augmented_manifest_url = f'{s3_output_path}/{labeling_job_name}/manifests/output/output.manifest'\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    os.makedirs('od_output_data/', exist_ok=False)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree('od_output_data/')\n",
    "\n",
    "# now download the augmented manifest file and display first 3 lines\n",
    "!aws s3 cp $augmented_manifest_url od_output_data/\n",
    "augmented_manifest_file = 'od_output_data/output.manifest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first few lines of the manifest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 $augmented_manifest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all the annotated images. First, let's define a function that displays the local image file and draws over it the bounding boxes obtained via labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "def show_annotated_image(img_path, bboxes):\n",
    "    im = np.array(Image.open(img_path), dtype=np.uint8)\n",
    "    \n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    colors = cycle(['r', 'g', 'b', 'y', 'c', 'm', 'k', 'w'])\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        # Create a Rectangle patch\n",
    "        rect = patches.Rectangle((bbox['left'],bbox['top']),bbox['width'],bbox['height'],linewidth=1,edgecolor=next(colors),facecolor='none')\n",
    "\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read the augmented manifest (JSON lines format) line by line and display the first 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install jsonlines\n",
    "import jsonlines\n",
    "from itertools import islice\n",
    "\n",
    "with jsonlines.open(augmented_manifest_file, 'r') as reader:\n",
    "    for desc in islice(reader, 10):\n",
    "        img_url = desc['source-ref']\n",
    "        img_file = os.path.basename(img_url)\n",
    "        file_exists = os.path.isfile(img_file)\n",
    "\n",
    "        bboxes = desc[labeling_job_name]['annotations']\n",
    "        show_annotated_image(img_file, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='training'></a>\n",
    "## Training an Object Detection Model\n",
    "We are now ready to use the labeled dataset in order to train a Machine Learning model using the SageMaker [built-in Object Detection algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html).\n",
    "\n",
    "For this, we would need to split the full labeled dataset into a training and a validation datasets. Out of the total of 500 images we are going to use 400 for training and 100 for validation. The algorithm will use the first one to train the model and the latter to estimate the accuracy of the model, trained so far. The augmented manifest file from the previously run full labeling job was included in the original zip archive as `output.manifest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with jsonlines.open('output.manifest', 'r') as reader:\n",
    "    lines = list(reader)\n",
    "    # Shuffle data in place.\n",
    "    np.random.shuffle(lines)\n",
    "    \n",
    "dataset_size = len(lines)\n",
    "num_training_samples = round(dataset_size*0.8)\n",
    "\n",
    "train_data = lines[:num_training_samples]\n",
    "validation_data = lines[num_training_samples:]\n",
    "\n",
    "augmented_manifest_filename_train = 'train.manifest'\n",
    "\n",
    "with open(augmented_manifest_filename_train, 'w') as f:\n",
    "    for line in train_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "\n",
    "augmented_manifest_filename_validation = 'validation.manifest'\n",
    "\n",
    "with open(augmented_manifest_filename_validation, 'w') as f:\n",
    "    for line in validation_data:\n",
    "        f.write(json.dumps(line))\n",
    "        f.write('\\n')\n",
    "        \n",
    "print(f'training samples: {num_training_samples}, validation samples: {len(lines)-num_training_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's upload the two manifest files to S3 in preparation for training. We will use the same bucket you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx_training = PREFIX + '/training' if PREFIX else 'training'\n",
    "# Defines paths for use in the training job request.\n",
    "s3_train_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_train)\n",
    "s3_validation_data_path = 's3://{}/{}/{}'.format(BUCKET, pfx_training, augmented_manifest_filename_validation)\n",
    "\n",
    "!aws s3 cp train.manifest s3://$BUCKET/$pfx_training/\n",
    "!aws s3 cp validation.manifest s3://$BUCKET/$pfx_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to kick off the training. We will do it from the SageMaker console, by following the [steps shown below](#console). Alternatively, you can just run this code in a new cell using SageMaker Python SDK:\n",
    "### Code option\n",
    "\n",
    "```python\n",
    "import time\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "training_image = sagemaker.amazon.amazon_estimator.get_image_uri(\n",
    "    boto3.Session().region_name, 'object-detection', repo_version='latest')\n",
    "s3_output_path = 's3://{}/{}/output'.format(BUCKET, pfx_training)\n",
    "\n",
    "# Create unique job name\n",
    "training_job_name = 'bees-detection-resnet'\n",
    "\n",
    "training_params = \\\n",
    "    {\n",
    "        \"AlgorithmSpecification\": {\n",
    "            # NB. This is one of the named constants defined in the first cell.\n",
    "            \"TrainingImage\": training_image,\n",
    "            \"TrainingInputMode\": \"Pipe\"\n",
    "        },\n",
    "        \"RoleArn\": role,\n",
    "        \"OutputDataConfig\": {\n",
    "            \"S3OutputPath\": s3_output_path\n",
    "        },\n",
    "        \"ResourceConfig\": {\n",
    "            \"InstanceCount\": 1,\n",
    "            \"InstanceType\": \"ml.p2.xlarge\",\n",
    "            \"VolumeSizeInGB\": 50\n",
    "        },\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"HyperParameters\": {  # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n",
    "            \"base_network\": \"resnet-50\",\n",
    "            \"use_pretrained_model\": \"1\",\n",
    "            \"num_classes\": \"1\",\n",
    "            \"mini_batch_size\": \"1\",\n",
    "            \"epochs\": \"100\",\n",
    "            \"learning_rate\": \"0.001\",\n",
    "            \"lr_scheduler_step\": \"\",\n",
    "            \"lr_scheduler_factor\": \"0.1\",\n",
    "            \"optimizer\": \"sgd\",\n",
    "            \"momentum\": \"0.9\",\n",
    "            \"weight_decay\": \"0.0005\",\n",
    "            \"overlap_threshold\": \"0.5\",\n",
    "            \"nms_threshold\": \"0.45\",\n",
    "            \"image_shape\": \"300\",\n",
    "            \"label_width\": \"350\",\n",
    "            \"num_training_samples\": str(num_training_samples)\n",
    "        },\n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 86400\n",
    "        },\n",
    "        \"InputDataConfig\": [\n",
    "            {\n",
    "                \"ChannelName\": \"train\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
    "                        \"S3Uri\": s3_train_data_path,\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                        # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                        \"AttributeNames\": ['source-ref', 'bees-500']\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"application/x-recordio\",\n",
    "                \"RecordWrapperType\": \"RecordIO\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            },\n",
    "            {\n",
    "                \"ChannelName\": \"validation\",\n",
    "                \"DataSource\": {\n",
    "                    \"S3DataSource\": {\n",
    "                        \"S3DataType\": \"AugmentedManifestFile\",  # NB. Augmented Manifest\n",
    "                        \"S3Uri\": s3_validation_data_path,\n",
    "                        \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                        # NB. This must correspond to the JSON field names in your augmented manifest.\n",
    "                        \"AttributeNames\": ['source-ref', 'bees-500']\n",
    "                    }\n",
    "                },\n",
    "                \"ContentType\": \"application/x-recordio\",\n",
    "                \"RecordWrapperType\": \"RecordIO\",\n",
    "                \"CompressionType\": \"None\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Now we create the SageMaker training job.\n",
    "client = boto3.client(service_name='sagemaker')\n",
    "client.create_training_job(**training_params)\n",
    "\n",
    "# Confirm that the training job has started\n",
    "status = client.describe_training_job(TrainingJobName=training_job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "```\n",
    "\n",
    "<a name='console'></a>\n",
    "\n",
    "### Console option\n",
    "\n",
    "#### Step 1: SageMaker Training Job wizard\n",
    "\n",
    "First, click on the \"Training Jobs\" link in the SageMaker Nav bar.\n",
    "\n",
    "<img src=\"guide/nav_training_jobs.png\" width=\"300\" border=\"3\">\n",
    "\n",
    "Then, click on \"Create training job\" button.\n",
    "\n",
    "<img src=\"guide/create_training_job.png\" width=\"800\" border=\"3\">\n",
    "\n",
    "#### Step 2: Enter basic details\n",
    "\n",
    "Next, enter training job name, e.g. `bees-detection-resnet`, create or select an existing role, leave the default `Amazon SageMaker built-in algorithm` and choose `Object Detection` from the drop-down.\n",
    "\n",
    "<img src=\"guide/job_details_1.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Scroll down to select Pipe input mode (data is streamed from S3) and pick the `p2.xlarge` instance for training.\n",
    "\n",
    "<img src=\"guide/job_details_2.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "#### Step 3: Hyperparameters\n",
    "\n",
    "Scroll down even further to specify algorithm hyperparameters. The meaning of hyperparameters is available in [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-api-config.html). First, we are going to select `resnet-50` instead of the default `vgg-16` for the `base_network`, then select `1` (means true) for `use_pretrained_model` (we will start with a pretrained model), enter 1 for `num_classes` (we only have one class - bees), choose `100` for the number of training `epochs` and change `mini_batch_size` to `1` (one image per batch).\n",
    "\n",
    "<img src=\"guide/job_details_3.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Keep scrolling to specify more hyperparameters. Enter the `num_training_samples` - it was 400 for our dataset. Then fill the remaining hyperparameters related to early stopping condition as shown below. This means that we might stop training even before the 100 epochs are completed (but not earlier than 50 epochs) when the validation accuracy metric mAP fails to improve by 0.01 in 5 epochs.\n",
    "\n",
    "<img src=\"guide/job_details_4.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "#### Step 4: Specify training/validation datasets\n",
    "\n",
    "Next, let's specify where the training algorithm will read the training and validation datasets (aka channels). Fill in the information as shown in the screenshots below. The S3 location of manifests files should be the same that you used above when uploading them to S3 - replace `s3://sagemaker-remars/training/` with your own prefix.\n",
    "\n",
    "<img src=\"guide/job_details_5.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "Click `Add Channel` in order to add a validation channel. Make sure to type the name `validation` in the `Channel name` box.\n",
    "\n",
    "<img src=\"guide/job_details_6.png\" width=\"500\" border=\"3\">\n",
    "\n",
    "#### Step 5: Kick off the training job\n",
    "\n",
    "Now, let's specify the S3 output location where the training algorithm will save the trained model artifacts. Once again your bucket and prefix will be different. Finally, click the `Create training job` button to start the training!\n",
    "\n",
    "<img src=\"guide/job_details_7.png\" width=\"800\" border=\"3\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the progess of the training job, you can refresh the console or repeatedly evaluate the following cell. When the training job status reads `'Completed'`, move on to the next part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
    "# In the above console screenshots the job name was 'bees-detection-resnet'.\n",
    "# But if you used Python to kick off the training job,\n",
    "# then 'training_job_name' is already set, so you can comment out the line below.\n",
    "training_job_name = 'bees-detection-resnet'\n",
    "##### REPLACE WITH YOUR OWN TRAINING JOB NAME\n",
    "\n",
    "training_info = client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "print(\"Training job status: \", training_info['TrainingJobStatus'])\n",
    "print(\"Secondary status: \", training_info['SecondaryStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='review_training'></a>\n",
    "\n",
    "## Review of Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the SageMaker model out of model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name = training_job_name + '-model' + timestamp\n",
    "\n",
    "training_image = training_info['AlgorithmSpecification']['TrainingImage']\n",
    "model_data = training_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "create_model_response = client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = training_job_name + '-epc' + timestamp\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.t2.medium',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "The next cell creates an endpoint that can be validated and incorporated into production applications. This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = training_job_name + '-ep' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = client.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "# get the status of the endpoint\n",
    "response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invoke the deployed endpoint to detect bees in the 10 test images that were inside the `test` folder in `dataset.zip` which you downloaded at the beginning of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "test_images = glob.glob('test/*')\n",
    "print(*test_images, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that converts the prediction array returned by our endpoint to the bounding box structure expected by our image display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_bbox_data(image_path, prediction):\n",
    "    class_id, confidence, xmin, ymin, xmax, ymax = prediction\n",
    "    width, height = Image.open(image_path).size\n",
    "    bbox_data = {'class_id': class_id,\n",
    "               'height': (ymax-ymin)*height,\n",
    "               'width': (xmax-xmin)*width,\n",
    "               'left': xmin*width,\n",
    "               'top': ymin*height}\n",
    "    return bbox_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for each of the test images, the following cell transforms the image into the appropriate format for realtime prediction, repeatedly calls the endpoint, receives back the prediction, and displays the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Call SageMaker endpoint to obtain predictions\n",
    "def get_predictions_for_img(runtime_client, endpoint_name, img_path):\n",
    "    with open(img_path, 'rb') as f:\n",
    "        payload = f.read()\n",
    "        payload = bytearray(payload)\n",
    "\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                       ContentType='application/x-image', \n",
    "                                       Body=payload)\n",
    "\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "client.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')\n",
    "\n",
    "for test_image in test_images:\n",
    "    result = get_predictions_for_img(runtime_client, endpoint_name, test_image)\n",
    "    confidence_threshold = .48\n",
    "    best_n = 2\n",
    "    # display the best n predictions with confidence > confidence_threshold\n",
    "    predictions = [prediction for prediction in result['prediction'] if prediction[1] > confidence_threshold]\n",
    "    predictions.sort(reverse=True, key = lambda x: x[1])\n",
    "    bboxes = [prediction_to_bbox_data(test_image, prediction) for prediction in predictions[:best_n]]\n",
    "    show_annotated_image(test_image, bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='model_tuning'></a>\n",
    "## Model Tuning\n",
    "\n",
    "When you configured the training job you needed to add many hyperparameters that affect the performance of the algorithm and the quality of the resulting model. But how do you pick the right hyperparameters?\n",
    "\n",
    "If you have experience with the specific algorithm and understand the innerworkings of it, you may already have a good sense of appropriate values. But even then, it's impossible to know the exact best value of each hyperparameter. Often you can zero in on the best values by trying many different combination of values, effectively searching in the hyperparameter space. SageMaker makes this extremely easy with the Model Tuning feature, also known as Hyperparameter Optimization (or HPO). With Model Tuning you simply decide which of the hyperparameters you are not sure about and specify the range of values for each that SageMaker needs to explore. Let's see again how this can be accomplished via the console.\n",
    "\n",
    "#### Step 1: SageMaker Training Job wizard\n",
    "\n",
    "First, click on the \"Training Jobs\" link in the SageMaker Nav bar.\n",
    "\n",
    "<img src=\"guide/nav_hpo_tuning_jobs.png\" width=\"300\" border=\"3\">\n",
    "\n",
    "Then, click on \"Create hyperparameter tuning job\" button.\n",
    "\n",
    "<img src=\"guide/create_hpo_job.png\" width=\"800\" border=\"3\">\n",
    "\n",
    "#### Step 2: SageMaker Hyperparameter Tuning Job wizard: objective metric\n",
    "\n",
    "You have now entered the Hyperparameter Tuning Job wizard and for the most part you would need to make the same selections as you did when you created the training job or leave default selections. When you get to selecting the so-called `objective metric` you want to make the following choices:\n",
    "\n",
    "<img src=\"guide/hpo_obj_metric.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "#### Step 3: SageMaker Hyperparameter Tuning Job wizard: hyperparameter ranges\n",
    "\n",
    "Next, we need to tell SageMaker which hyperparameters we are fixing (leaving `Static`) and which we'd like to optimize. Here is the [relevant documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tuning.html) for the Object Detection algorithm. For this exercise, we will be optimizing `learning_rate`, `mini_batch_size` and `optimizer`, and leave the others `Static`.\n",
    "\n",
    "<img src=\"guide/hpo_param_ranges.png\" width=\"600\" border=\"3\">\n",
    "<img src=\"guide/hpo_param_ranges_2.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "#### Step 4: SageMaker Hyperparameter Tuning Job wizard: Input and Output locations\n",
    "\n",
    "Next, specify the training/validation data sources and output location - all with the same values as regular training.\n",
    "\n",
    "#### Step 5: SageMaker Hyperparameter Tuning Job wizard: Resource configuration and limits\n",
    "\n",
    "And now, specify the instance type for each training job (same `p2.xlarge` works), number of instances each training job is going to run on and, finally, the limit on how many training jobs will be run in total (this is a way to control the costs) as well as the limit on how many parallel jobs you can run. The latter parameter controls how many instances will be used concurrently at any one time. So 1 instance per training job with 3 parallel jobs maximum would mean at most 3 `p2.xlarge` instances being used concurrently while the hyperparameter tuning job is running.\n",
    "\n",
    "<img src=\"guide/hpo_resources_limits.png\" width=\"600\" border=\"3\">\n",
    "\n",
    "Finally, just press the `Create job` button to kick off training. Once the HPO jobs are all finished, you can use the console to find the one job that resulted in the best (highest) value for `validation:mAP`. You can then use the model artifacts as previously described when we trained the first model (without HPO), and deploy them to an endpoint to verify the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='cleanup'></a>\n",
    "## Cleanup\n",
    "\n",
    "At the end of the lab we would like to delete the real-time endpoint, as keeping a real-time endpoint around while being idle is costly and wasteful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
